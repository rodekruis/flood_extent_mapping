{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar-based flood mapping\n",
    "*Following the procedure from [UN-SPIDER radar-based flood mapping](https://github.com/UN-SPIDER/radar-based-flood-mapping)*\n",
    "\n",
    "**DOING**: \n",
    "- Figuring out why the processing step doesn't run smoothly with large AOI's. \n",
    "- So far I've removed code for the \"Start processing\" button as it should process automatically. \n",
    "- The button was called by the getScene fuction, which checks if there is an AOI - somehow it cannot always find one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "\n",
    "Please specify in the code cell below i) the polarisation to be processed, ii) whether data shall be downloaded from the <a href=\"https://scihub.copernicus.eu/\">Copernicus Open Access Hub</a> with respective sensing period and login details, and iii) whether intermediate results should be plotted during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarisations to be processed\n",
    "polarisations = 'VH'                              # 'VH', 'VV', 'both'\n",
    "\n",
    "# download image from Copernicus Open Access Hub\n",
    "download = {\n",
    "    'imageDownload'     : True,                   # 'True', 'False'\n",
    "    'period_start'      : [2022, 1, 15],           # format: [Year, Month, Day] DAY AFTER FLOOD HAPPENED -> code will automatically search the most recent image within given period\n",
    "    'period_stop'       : [2022, 1, 25],           # format: [Year, Month, Day] WEEK AFTER FLOOD HAPPENED\n",
    "    'username'          : 'username',             # username for login\n",
    "    'password'          : 'password'              # password for login\n",
    "}\n",
    "\n",
    "# show intermediate results if set to 'True'\n",
    "plotResoluts = True                               # 'True', 'False'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "This section loads relevant Python modules for the following analysis and initializes basic functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     37,
     85
    ]
   },
   "outputs": [],
   "source": [
    "# Click to run\n",
    "\n",
    "#####################################################\n",
    "###################### IMPORTS ######################\n",
    "#####################################################\n",
    "\n",
    "# MODULE                                      # DESCRIPTION\n",
    "import sys\n",
    "import matplotlib.pyplot as plt               # create visualizations\n",
    "import numpy as np                            # scientific comupting\n",
    "import json                                   # JSON encoder and decoder\n",
    "import glob                                   # data access\n",
    "import os                                     # data access\n",
    "import ipywidgets                             # interactive UI controls\n",
    "import time                                   # time assessment\n",
    "import shutil                                 # file operations\n",
    "import ipyleaflet                             # visualization\n",
    "import geopandas                              # data analysis and manipulation\n",
    "import pandas as pd####\n",
    "import snappy                                 # SNAP Python interface\n",
    "import jpy                                    # Python-Java bridge\n",
    "import skimage.filters                        # threshold calculation\n",
    "import functools                              # higher-order functions and operations\n",
    "from ipyfilechooser import FileChooser        # file chooser widget\n",
    "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt  # interface to Open Access Hub\n",
    "from datetime import date                     # dates, times and intervalls\n",
    "from IPython.display import display           # visualization\n",
    "from osgeo import ogr, gdal, osr              # data conversion\n",
    "from zipfile import ZipFile                   # file management\n",
    "from collections import OrderedDict\n",
    "\n",
    "####################################################\n",
    "############### FUNCTION DEFINITIONS ###############\n",
    "####################################################\n",
    "\n",
    "# Function looks for AOI file, converts to GeoJSON if not given and returns GeoJSON\n",
    "def readJSONFromAOI(path):\n",
    "    # check for GeoJSON file in 'AOI' subfolder\n",
    "    if len(glob.glob('%s/*.geojson' % path)) == 1:\n",
    "        file = glob.glob('%s/*.geojson' % path)[0]\n",
    "    elif len(glob.glob('%s/*.json' % path)) == 1:\n",
    "        file = glob.glob('%s/*.json' % path)[0]\n",
    "\n",
    "    # convert SHP to GeoJSON if no JSON is given\n",
    "    elif len(glob.glob('%s/*.shp' % path)) == 1:\n",
    "        file_name = os.path.splitext(glob.glob('%s/*.shp' % path)[0])[0].split('/')[-1]\n",
    "        shp_file = geopandas.read_file(glob.glob('%s/*.shp' % path)[0])\n",
    "        shp_file.to_file('%s/%s.json' % (path, file_name), driver='GeoJSON')\n",
    "        file = glob.glob('%s/*.json' % path)[0]\n",
    "\n",
    "    # convert KML to GeoJSON if no JSON or SHP is given\n",
    "    elif len(glob.glob('%s/*.kml' % path)) == 1:\n",
    "        file_name = os.path.splitext(glob.glob('%s/*.kml' % path)[0])[0].split('/')[-1]\n",
    "        kml_file = gdal.OpenEx(glob.glob('%s/*.kml' % path)[0])\n",
    "        ds = gdal.VectorTranslate('%s/%s.json' % (path, file_name), kml_file, format='GeoJSON')\n",
    "        del ds\n",
    "        file = glob.glob('%s/*.json' % path)[0]\n",
    "\n",
    "    # convert KMZ to JSON if no JSON, SHP, or KML is given\n",
    "    elif len(glob.glob('%s/*.kmz' % path)) == 1:\n",
    "        # open KMZ file and extract data\n",
    "        with ZipFile(glob.glob('%s/*.kmz' % path)[0], 'r') as kmz:\n",
    "            folder = os.path.splitext(glob.glob('%s/*.kmz' % path)[0])[0]\n",
    "            kmz.extractall(folder)\n",
    "        # convert KML to GeoJSON if extracted folder contains one KML file\n",
    "        if len(glob.glob('%s/*.kml' % folder)) == 1:\n",
    "            kml_file = gdal.OpenEx(glob.glob('%s/*.kml' % folder)[0])\n",
    "            ds = gdal.VectorTranslate('%s/%s.json' % (path, folder.split('/')[-1]), kml_file, format='GeoJSON')\n",
    "            del ds\n",
    "            file = glob.glob('%s/*.json' % path)[0]\n",
    "            # remove unzipped KMZ directory and data\n",
    "            shutil.rmtree(folder)\n",
    "    # allow to upload AOI file or manually draw AOI if no file was found\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "\n",
    "    # open JSON file and store data\n",
    "    with open(file, 'r') as f:\n",
    "        data_json = json.load(f)\n",
    "\n",
    "    return data_json\n",
    "\n",
    "# plot band and histogram of 'Band'-type input and threshold\n",
    "# SNAP API: https://step.esa.int/docs/v6.0/apidoc/engine/\n",
    "def plotBand(band, threshold, binary=False):\n",
    "    # color stretch\n",
    "    vmin, vmax = 0, 1\n",
    "    # read pixel values\n",
    "    w = band.getRasterWidth()\n",
    "    h = band.getRasterHeight()\n",
    "    band_data = np.zeros(w * h, np.float32)\n",
    "    band.readPixels(0, 0, w, h, band_data)\n",
    "    band_data.shape = h, w\n",
    "    # color stretch\n",
    "    if binary:\n",
    "        cmap = plt.get_cmap('binary')\n",
    "    else:\n",
    "        vmin = np.percentile(band_data, 2.5)\n",
    "        vmax = np.percentile(band_data, 97.5)\n",
    "        cmap = plt.get_cmap('gray')\n",
    "    # plot band\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,6))\n",
    "    ax1.imshow(band_data, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax1.set_title(band.getName())\n",
    "    # plot histogram\n",
    "    band_data.shape = h * w \n",
    "    ax2.hist(np.asarray(band_data[band_data != 0], dtype='float'), bins=2048)\n",
    "    ax2.axvline(x=threshold, color='r')\n",
    "    ax2.set_title('Histogram: %s' % band.getName())\n",
    "    \n",
    "    for ax in fig.get_axes():\n",
    "        ax.label_outer()\n",
    "\n",
    "\n",
    "####################################################\n",
    "####################### CODE #######################\n",
    "####################################################   \n",
    "        \n",
    "# get current working directory\n",
    "directory = os.getcwd()\n",
    "\n",
    "print('Initialization done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find available satellite image\n",
    "Finds available satellite images between `period_start` and `period_stop` and selects the image closest to the `period_start` date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     7,
     57,
     65,
     79
    ]
   },
   "outputs": [],
   "source": [
    "# Click to run\n",
    "####################################################\n",
    "############### FUNCTION DEFINITIONS ###############\n",
    "####################################################\n",
    "\n",
    "# search for and display available Sentinel-1 tiles\n",
    "def queri(footprint):\n",
    "    # print status\n",
    "    print('Loading...', flush=True)\n",
    "    # search Copernicus Open Access Hub for products with regard to input footprint and sensing period\n",
    "    try:\n",
    "        products = api.query(footprint,\n",
    "                             date = (date(download['period_start'][0], download['period_start'][1], download['period_start'][2]),\n",
    "                                     date(download['period_stop'][0], download['period_stop'][1], download['period_stop'][2])),\n",
    "                             platformname = 'Sentinel-1',\n",
    "                             producttype = 'GRD')\n",
    "        print('Successfully connected to Copernicus Open Access Hub.\\n', flush=True)\n",
    "    except:\n",
    "        sys.exit('\\nLogin data not valid. Please change username and/or password.')\n",
    "    \n",
    "    # convert to dataframe\n",
    "    products_df = api.to_dataframe(products).sort_values('ingestiondate', ascending=[True])\n",
    "    \n",
    "    # THIS IS NOT WOKRING raise warning that no image is available in given sensing period\n",
    "    products_json = api.to_geojson(products)\n",
    "    if not products_json['features']:\n",
    "        sys.exit('\\nNo Sentinel-1 images available. Please change sensing period in user input section.')\n",
    "        \n",
    "    # get product closest to flood date product by id\n",
    "    firstproduct_id = products_df['uuid'][0]\n",
    "    print('ProductID of Sentinel-1 image taken closest to flood date: ' + firstproduct_id)\n",
    "    firstproduct = products.popitem(last=True)\n",
    "    firstproductdict = OrderedDict([firstproduct])\n",
    "        \n",
    "    # convert to GeoJSON for plot\n",
    "    products_json = api.to_geojson(firstproductdict) \n",
    "    geo_json = ipyleaflet.GeoJSON(data = products_json,\n",
    "                                  name = 'S1 tiles',\n",
    "                                  style = {'color' : 'royalblue'},\n",
    "                                  hover_style = {'fillOpacity' : 0.4})\n",
    "    download_map.add_layer(geo_json)\n",
    "\n",
    "    productinfo = products_df[[\"filename\", \"size\", \"beginposition\", \"endposition\", \"ingestiondate\", \"footprint\"]]\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    display(productinfo.head(1))\n",
    "    return products_df['uuid'][0]\n",
    "    \n",
    "####################################################\n",
    "####################### CODE #######################\n",
    "####################################################\n",
    "\n",
    "# check user input whether image download is requested\n",
    "if download['imageDownload']:\n",
    "    # connect to the API\n",
    "    api = SentinelAPI(download['username'], download['password'], 'https://scihub.copernicus.eu/dhus')\n",
    "    # create map with search functionality\n",
    "    download_map = ipyleaflet.Map(zoom = 6)\n",
    "    download_map.add_control(ipyleaflet.SearchControl(\n",
    "        position = 'topleft',\n",
    "        url = 'https://nominatim.openstreetmap.org/search?format=json&q={s}',\n",
    "        zoom = 5))\n",
    "    download_map.add_control(ipyleaflet.ScaleControl(position='bottomleft'))\n",
    "    display(download_map)\n",
    "      \n",
    "    # check whether AOI file is given and convert to JSON in order to show AOI on map\n",
    "    try:\n",
    "        # calls readJSONfromAOI function to get GeoJSON from either JSON, SHP, or KMZ file\n",
    "        data_json = readJSONFromAOI('%s/AOI' % directory)\n",
    "        # show AOI on map according to JSON data\n",
    "        aoi = ipyleaflet.GeoJSON(data = data_json, style = {'color' : 'green'})\n",
    "        try:\n",
    "            # GeoJSON format if KMZ is given\n",
    "            download_map.center = (aoi.data['features'][0]['geometry']['coordinates'][0][0][0][1],\n",
    "                                   aoi.data['features'][0]['geometry']['coordinates'][0][0][0][0])\n",
    "        except:\n",
    "            # GeoJSON format if JSON or SHP is given\n",
    "            download_map.center = (aoi.data['features'][0]['geometry']['coordinates'][0][0][1],\n",
    "                                   aoi.data['features'][0]['geometry']['coordinates'][0][0][0])\n",
    "        download_map.add_layer(aoi)\n",
    "        # search for available Sentinel-1 tiles according to JSON data\n",
    "        footprint = geojson_to_wkt(data_json)\n",
    "        tile_id = queri(footprint)\n",
    "\n",
    "    # if no AOI is given, it needs to be uploaded\n",
    "    except FileNotFoundError:\n",
    "        print('No area of interest found. Please add one to the AOI map')\n",
    "\n",
    "print('Searching done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download satellite image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get product information\n",
    "product_info = api.get_product_odata(tile_id)\n",
    "# check whether product is available\n",
    "if product_info['Online']:\n",
    "    # check if input folder exists, if not create input folder\n",
    "    input_path = os.path.join(directory, 'input')\n",
    "    if not os.path.isdir(input_path):\n",
    "        os.mkdir(input_path)\n",
    "    # change into 'input' subfolder for storing product\n",
    "    os.chdir(input_path)\n",
    "    # status update\n",
    "    print('\\nProduct %s is online. Starting download.' % tile_id, flush=True)\n",
    "    # download product\n",
    "    api.download(tile_id)\n",
    "    # change back to previous working directory\n",
    "    os.chdir(directory)\n",
    "# error message when product is not available\n",
    "else:\n",
    "    print('\\nProduct %s is not online. Must be requested manually.\\n' % tile_id, flush=True)\n",
    "    \n",
    "print('Downloading done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     7,
     61,
     98,
     118,
     129
    ]
   },
   "outputs": [],
   "source": [
    "# Click to run\n",
    "\n",
    "####################################################\n",
    "############### FUNCTION DEFINITIONS ###############\n",
    "####################################################\n",
    "\n",
    "# create S1 product\n",
    "def getScene(path):\n",
    "    # set correct path of input file and create S1 product\n",
    "    if len(files) is 1:\n",
    "        file_path = path\n",
    "    else:\n",
    "        file_path = path.selected\n",
    "    S1_source = snappy.ProductIO.readProduct(file_path)\n",
    "\n",
    "    # read geographic coordinates from Sentinel-1 image meta data\n",
    "    meta_data = S1_source.getMetadataRoot().getElement('Abstracted_Metadata')\n",
    "    # refines center of map according to Sentinel-1 image\n",
    "    center = (meta_data.getAttributeDouble('centre_lat'), meta_data.getAttributeDouble('centre_lon'))\n",
    "    locations = [[{'lat' : meta_data.getAttributeDouble('first_near_lat'), 'lng' : meta_data.getAttributeDouble('first_near_long')},\n",
    "                  {'lat' : meta_data.getAttributeDouble('last_near_lat'),  'lng' : meta_data.getAttributeDouble('last_near_long')},\n",
    "                  {'lat' : meta_data.getAttributeDouble('last_far_lat'),   'lng' : meta_data.getAttributeDouble('last_far_long')},\n",
    "                  {'lat' : meta_data.getAttributeDouble('first_far_lat'),  'lng' : meta_data.getAttributeDouble('first_far_long')}]]\n",
    "\n",
    "    # creates interactive map\n",
    "    basic_map = ipyleaflet.Map(center = center, zoom = 7.5)\n",
    "    # defines fixed polygon illustrating Sentinel-1 image\n",
    "    polygon_fix = ipyleaflet.Polygon(locations = locations, color='royalblue')\n",
    "    basic_map.add_layer(polygon_fix)\n",
    "    # displays map\n",
    "    basic_map.add_control(ipyleaflet.ScaleControl(position='bottomleft'))\n",
    "    display(basic_map)\n",
    "    \n",
    "    # check whether AOI file is given and convert to JSON in order to show AOI on map\n",
    "    try:\n",
    "        # calls readJSONfromAOI function to get GeoJSON from either JSON, SHP, or KMZ file\n",
    "        data_json = readJSONFromAOI('%s/AOI' % directory)\n",
    "        # show AOI on map according to JSON data\n",
    "        basic_map.add_layer(ipyleaflet.GeoJSON(data = data_json, style = {'color' : 'green'}))\n",
    "        # apply subset according to JSON data\n",
    "        footprint = geojson_to_wkt(data_json)\n",
    "        # run processing process\n",
    "        processing(S1_source, footprint)\n",
    "        \n",
    "    except:\n",
    "        print('No AOI found')\n",
    "    \n",
    "# calculate and return threshold of 'Band'-type input\n",
    "# SNAP API: https://step.esa.int/docs/v6.0/apidoc/engine/\n",
    "def getThreshold(S1_band):\n",
    "    # read band\n",
    "    w = S1_band.getRasterWidth()\n",
    "    h = S1_band.getRasterHeight()\n",
    "    band_data = np.zeros(w * h, np.float32)\n",
    "    S1_band.readPixels(0, 0, w, h, band_data)\n",
    "    band_data.shape = h * w\n",
    "\n",
    "    # calculate threshold using Otsu method\n",
    "    threshold_otsu = skimage.filters.threshold_otsu(band_data)\n",
    "    # calculate threshold using minimum method\n",
    "    threshold_minimum = skimage.filters.threshold_minimum(band_data)\n",
    "    # get number of pixels for both thresholds\n",
    "    numPixOtsu = len(band_data[abs(band_data - threshold_otsu) < 0.1])\n",
    "    numPixMinimum = len(band_data[abs(band_data - threshold_minimum) < 0.1])\n",
    "\n",
    "    # if number of pixels at minimum threshold is less than 1% of number of pixels at Otsu threshold\n",
    "    if abs(numPixMinimum/numPixOtsu) < 0.001:\n",
    "        # adjust band data according\n",
    "        if threshold_otsu < threshold_minimum:\n",
    "            band_data = band_data[band_data < threshold_minimum]\n",
    "            threshold_minimum = skimage.filters.threshold_minimum(band_data)\n",
    "        else:\n",
    "            band_data = band_data[band_data > threshold_minimum]\n",
    "            threshold_minimum = skimage.filters.threshold_minimum(band_data)\n",
    "    \n",
    "        numPixMinimum = len(band_data[abs(band_data - threshold_minimum) < 0.1])\n",
    "\n",
    "    # select final threshold\n",
    "    if abs(numPixMinimum/numPixOtsu) < 0.001:\n",
    "        threshold = threshold_otsu\n",
    "    else:\n",
    "        threshold = threshold_minimum\n",
    "\n",
    "    return threshold\n",
    "\n",
    "# calculate binary mask of 'Product'-type intput with respect expression in string array\n",
    "def binarization(S1_product, expressions):\n",
    "\n",
    "    BandDescriptor = jpy.get_type('org.esa.snap.core.gpf.common.BandMathsOp$BandDescriptor')\n",
    "    targetBands = jpy.array('org.esa.snap.core.gpf.common.BandMathsOp$BandDescriptor', len(expressions))\n",
    "\n",
    "    # loop through bands\n",
    "    for i in range(len(expressions)):\n",
    "        targetBand = BandDescriptor()\n",
    "        targetBand.name = '%s' % S1_product.getBandNames()[i]\n",
    "        targetBand.type = 'float32'\n",
    "        targetBand.expression = expressions[i]\n",
    "        targetBands[i] = targetBand\n",
    "    \n",
    "    parameters = snappy.HashMap()\n",
    "    parameters.put('targetBands', targetBands)    \n",
    "    mask = snappy.GPF.createProduct('BandMaths', parameters, S1_product)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# processing steps\n",
    "def processing(S1_source, footprint):\n",
    "    \n",
    "    # Subset operator\n",
    "    parameters = snappy.HashMap()\n",
    "    parameters.put('copyMetadata', True)\n",
    "    geom = snappy.WKTReader().read(footprint)\n",
    "    parameters.put('geoRegion', geom)\n",
    "    parameters.put('sourceBands', sourceBands)\n",
    "    S1_crop = snappy.GPF.createProduct('Subset', parameters, S1_source)\n",
    "    # status update\n",
    "    print('\\nSubset successfully generated.\\n', flush=True)\n",
    "    \n",
    "    # Apply-Orbit-File operator\n",
    "    print('1. Apply Orbit File:          ', end='', flush=True)\n",
    "    start_time = time.time()\n",
    "    parameters = snappy.HashMap()\n",
    "    # continue with calculation in case no orbit file is available yet\n",
    "    parameters.put('continueOnFail', True)\n",
    "    S1_Orb = snappy.GPF.createProduct('Apply-Orbit-File', parameters, S1_crop)\n",
    "    print('--- %.2f  seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "    # ThermalNoiseRemoval operator\n",
    "    print('2. Thermal Noise Removal:     ', end='', flush=True)\n",
    "    start_time = time.time()\n",
    "    parameters = snappy.HashMap()\n",
    "    parameters.put('removeThermalNoise', True)\n",
    "    S1_Thm = snappy.GPF.createProduct('ThermalNoiseRemoval', parameters, S1_Orb)\n",
    "    print('--- %.2f  seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "    # Calibration operator\n",
    "    print('3. Radiometric Calibration:   ', end='', flush=True)\n",
    "    start_time = time.time()\n",
    "    parameters = snappy.HashMap()\n",
    "    parameters.put('outputSigmaBand', True)\n",
    "    S1_Cal = snappy.GPF.createProduct('Calibration', parameters, S1_Thm)\n",
    "    print('--- %.2f  seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "    # Speckle-Filter operator\n",
    "    print('4. Speckle Filtering:         ', end='', flush=True)\n",
    "    start_time = time.time()\n",
    "    parameters = snappy.HashMap()\n",
    "    parameters.put('filter', 'Lee')\n",
    "    parameters.put('filterSizeX', 5)\n",
    "    parameters.put('filterSizeY', 5)\n",
    "    S1_Spk = snappy.GPF.createProduct('Speckle-Filter', parameters, S1_Cal)\n",
    "    print('--- %.2f  seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "    # Conversion from linear to db operator\n",
    "    S1_Spk_db = snappy.GPF.createProduct('LinearToFromdB', snappy.HashMap(), S1_Spk)\n",
    "\n",
    "    # Terrain-Correction operator\n",
    "    print('5. Terrain Correction:        ', end='', flush=True)\n",
    "    parameters = snappy.HashMap()\n",
    "    parameters.put('demName', 'SRTM 1Sec HGT')\n",
    "    parameters.put('demResamplingMethod', 'BILINEAR_INTERPOLATION')\n",
    "    parameters.put('imgResamplingMethod', 'NEAREST_NEIGHBOUR')\n",
    "    parameters.put('pixelSpacingInMeter', 10.0)\n",
    "    parameters.put('nodataValueAtSea', False)\n",
    "    parameters.put('saveSelectedSourceBand', True)\n",
    "    S1_TC = snappy.GPF.createProduct('Terrain-Correction', parameters, S1_Spk_db)\n",
    "    print('--- %.2f  seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "    # Binarization\n",
    "    print('6. Binarization:              ', end='', flush=True)\n",
    "    start_time = time.time()\n",
    "    # add GlobCover band\n",
    "    parameters = snappy.HashMap()\n",
    "    parameters.put('landCoverNames', 'GlobCover')\n",
    "    GlobCover = snappy.GPF.createProduct('AddLandCover', parameters, S1_TC)\n",
    "    # empty string array for binarization band maths expression(s)\n",
    "    expressions = ['' for i in range(S1_TC.getNumBands())]\n",
    "    # empty array for threshold(s)\n",
    "    thresholds = np.zeros(S1_TC.getNumBands())\n",
    "    # loop through bands\n",
    "    for i in range(S1_TC.getNumBands()):\n",
    "        # calculate threshold of band and store in float array\n",
    "        # use S1_Spk_db product for performance reasons. S1_TC causes 0-values\n",
    "        # which distort histogram and thus threshold result\n",
    "        thresholds[i] = getThreshold(S1_Spk_db.getBandAt(i))\n",
    "        # formulate expression according to threshold and store in string array\n",
    "        expressions[i] = 'if (%s < %s && land_cover_GlobCover != 210) then 1 else NaN' % (S1_TC.getBandNames()[i], thresholds[i])\n",
    "    # do binarization\n",
    "    S1_floodMask = binarization(GlobCover, expressions)\n",
    "    print('--- %.2f seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "    # Speckle-Filter operator\n",
    "    print('7. Speckle Filtering:         ', end='', flush=True)\n",
    "    start_time = time.time()\n",
    "    parameters = snappy.HashMap()\n",
    "    parameters.put('filter', 'Median')\n",
    "    parameters.put('filterSizeX', 5)\n",
    "    parameters.put('filterSizeY', 5)\n",
    "    # define flood mask as global for later access\n",
    "    global S1_floodMask_Spk\n",
    "    S1_floodMask_Spk = snappy.GPF.createProduct('Speckle-Filter', parameters, S1_floodMask)\n",
    "    print('--- %.2f  seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "    # output\n",
    "    if plotResoluts:\n",
    "        print('8. Plot:                      ', end='', flush=True)\n",
    "        start_time = time.time()\n",
    "        for i in range(S1_TC.getNumBands()):\n",
    "            plotBand(S1_TC.getBandAt(i), thresholds[i])\n",
    "        print('--- %.2f seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "####################################################\n",
    "####################### CODE #######################\n",
    "####################################################\n",
    "\n",
    "# filter required polarisation(s) and set output file name accordingly\n",
    "if polarisations == 'both':\n",
    "    sourceBands = 'Amplitude_VH,Intensity_VH,Amplitude_VV,Intensity_VV'\n",
    "    output_extensions   = 'processed_VHVV'\n",
    "elif polarisations == 'VH':\n",
    "    sourceBands = 'Amplitude_VH,Intensity_VH'\n",
    "    output_extensions   = 'processed_VH'\n",
    "elif polarisations == 'VV':\n",
    "    sourceBands = 'Amplitude_VV,Intensity_VV'\n",
    "    output_extensions   = 'processed_VV'\n",
    "\n",
    "# path of Sentinel-1 .zip input file\n",
    "input_path = os.path.join(directory, 'input')\n",
    "# empty string array to store Sentinel-1 files in 'input' subfolder\n",
    "files = []\n",
    "# add files to list\n",
    "for file in glob.glob1(input_path, '*.zip'):\n",
    "    files.append(file)\n",
    "# select input file and start processing if there is only one available Sentinel-1 file\n",
    "if len(files) == 1:\n",
    "    input_name = files[0]\n",
    "    print('Selected:  %s\\n' % input_name, flush=True)\n",
    "    # apply subset according to JSON data\n",
    "    getScene('%s/%s' % (input_path, input_name))\n",
    "# open dialogue to select input file if more or less than one is available\n",
    "else:\n",
    "    print('More or less than one Sentinel-1 files have been found. Please select.', flush=True)\n",
    "    fc = FileChooser(input_path)\n",
    "    fc.filter_pattern = '*.zip'\n",
    "    fc.register_callback(getScene)\n",
    "    display(fc)\n",
    "        \n",
    "print('Processing done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Export\n",
    "The processed flood mask is exported as GeoTIFF, SHP, KML, and GeoJSON and stored in the *'output'* subfolder. An interactive map shows the flood mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Click to run\n",
    "\n",
    "####################################################\n",
    "####################### CODE #######################\n",
    "####################################################\n",
    "\n",
    "print('Exporting...\\n', flush=True)\n",
    "# check if output folders exists, if not create folders\n",
    "output_path = os.path.join(directory, 'output')\n",
    "if not os.path.isdir(output_path):\n",
    "    os.mkdir(output_path)\n",
    "GeoTIFF_path = os.path.join(output_path, 'GeoTIFF')\n",
    "if not os.path.isdir(GeoTIFF_path):\n",
    "    os.mkdir(GeoTIFF_path)\n",
    "SHP_path = os.path.join(output_path, 'SHP')\n",
    "if not os.path.isdir(SHP_path):\n",
    "    os.mkdir(SHP_path)\n",
    "KML_path = os.path.join(output_path, 'KML')\n",
    "if not os.path.isdir(KML_path):\n",
    "    os.mkdir(KML_path)\n",
    "GeoJSON_path = os.path.join(output_path, 'GeoJSON')\n",
    "if not os.path.isdir(GeoJSON_path):\n",
    "    os.mkdir(GeoJSON_path)\n",
    "# get file name if file chooser was used\n",
    "if len(files) is not 1: input_name = fc.selected_filename\n",
    "\n",
    "# write output file as GeoTIFF\n",
    "print('1. GeoTIFF:                   ', end='', flush=True)\n",
    "start_time = time.time()\n",
    "snappy.ProductIO.writeProduct(S1_floodMask_Spk, '%s/%s_%s' % (GeoTIFF_path, os.path.splitext(input_name)[0], output_extensions), 'GeoTIFF')\n",
    "print('--- %.2f seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "# convert GeoTIFF to SHP\n",
    "print('2. SHP:                       ', end='', flush=True)\n",
    "start_time = time.time()\n",
    "# allow GDAL to throw Python exceptions\n",
    "gdal.UseExceptions()\n",
    "open_image = gdal.Open('%s/%s_%s.tif' % (GeoTIFF_path, os.path.splitext(input_name)[0], output_extensions))\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromWkt(open_image.GetProjectionRef())\n",
    "shp_driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "# empty string array for bands in GeoTIFF\n",
    "output_shp = ['' for i in range(open_image.RasterCount)]\n",
    "if open_image.RasterCount == 1:\n",
    "    output_shp[0] = '%s/%s_processed_%s' % (SHP_path, os.path.splitext(input_name)[0], polarisations)\n",
    "else:\n",
    "    VH_SHP_path = os.path.join(SHP_path, 'VH')\n",
    "    if not os.path.isdir(VH_SHP_path):\n",
    "        os.mkdir(VH_SHP_path)\n",
    "    VV_SHP_path = os.path.join(SHP_path, 'VV')\n",
    "    if not os.path.isdir(VV_SHP_path):\n",
    "        os.mkdir(VV_SHP_path)\n",
    "    output_shp[0] = '%s/%s_processed_VH' % (VH_SHP_path, os.path.splitext(input_name)[0])\n",
    "    output_shp[1] = '%s/%s_processed_VV' % (VV_SHP_path, os.path.splitext(input_name)[0])\n",
    "# loops through bands in GeoTIFF\n",
    "for i in range(open_image.RasterCount):\n",
    "    input_band = open_image.GetRasterBand(i+1)\n",
    "    output_shapefile = shp_driver.CreateDataSource(output_shp[i] + '.shp')\n",
    "    new_shapefile = output_shapefile.CreateLayer(output_shp[i], srs=srs)\n",
    "    new_shapefile.CreateField(ogr.FieldDefn('DN', ogr.OFTInteger))\n",
    "    gdal.Polygonize(input_band, input_band.GetMaskBand(), new_shapefile, 0, [], callback=None)\n",
    "    # filters attributes with values other than 1 (sould be NaN or respective value)\n",
    "    new_shapefile.SetAttributeFilter('DN != 1')\n",
    "    for feat in new_shapefile:\n",
    "        new_shapefile.DeleteFeature(feat.GetFID())\n",
    "    new_shapefile.SyncToDisk()\n",
    "print('--- %.2f seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "# convert SHP to KML\n",
    "print('3. KML:                       ', end='', flush=True)\n",
    "start_time = time.time()\n",
    "if open_image.RasterCount == 1:\n",
    "    shp_file = gdal.OpenEx('%s/%s_processed_%s.shp' % (SHP_path, os.path.splitext(input_name)[0], polarisations))\n",
    "    ds = gdal.VectorTranslate('%s/%s_processed_%s.kml' % (KML_path, os.path.splitext(input_name)[0], polarisations), shp_file, format='KML')\n",
    "    del ds\n",
    "else:\n",
    "    shp_file_VH = gdal.OpenEx('%s/%s_processed_VH.shp' % (VH_SHP_path, os.path.splitext(input_name)[0]))\n",
    "    ds_VH = gdal.VectorTranslate('%s/%s_processed_VH.kml' % (KML_path, os.path.splitext(input_name)[0]), shp_file_VH, format='KML')\n",
    "    del ds_VH\n",
    "    shp_file_VV = gdal.OpenEx('%s/%s_processed_VV.shp' % (VV_SHP_path, os.path.splitext(input_name)[0]))\n",
    "    ds_VV = gdal.VectorTranslate('%s/%s_processed_VV.kml' % (KML_path, os.path.splitext(input_name)[0]), shp_file_VV, format='KML')\n",
    "    del ds_VV\n",
    "print('--- %.2f seconds ---' % (time.time() - start_time), flush=True)\n",
    "\n",
    "# convert SHP to GeoJSON\n",
    "print('4. GeoJSON:                   ', end='', flush=True)\n",
    "start_time = time.time()\n",
    "if open_image.RasterCount == 1:\n",
    "    shp_file = geopandas.read_file('%s/%s_processed_%s.shp' % (SHP_path, os.path.splitext(input_name)[0], polarisations))\n",
    "    shp_file.to_file('%s/%s_processed_%s.json' % (GeoJSON_path, os.path.splitext(input_name)[0], polarisations), driver='GeoJSON')\n",
    "else:\n",
    "    shp_file_VH = geopandas.read_file('%s/%s_processed_VH.shp' % (VH_SHP_path, os.path.splitext(input_name)[0]))\n",
    "    shp_file_VH.to_file('%s/%s_processed_VH.json' % (GeoJSON_path, os.path.splitext(input_name)[0]), driver='GeoJSON')    \n",
    "    shp_file_VV = geopandas.read_file('%s/%s_processed_VV.shp' % (VV_SHP_path, os.path.splitext(input_name)[0]))\n",
    "    shp_file_VV.to_file('%s/%s_processed_VV.json' % (GeoJSON_path, os.path.splitext(input_name)[0]), driver='GeoJSON')\n",
    "print('--- %.2f seconds ---\\n' % (time.time() - start_time), flush=True)\n",
    "print('Files successfuly stored under %s.\\n' % output_path, flush=True)\n",
    "print('Data export done.')\n",
    "\n",
    "# plot results\n",
    "results_map = ipyleaflet.Map(zoom=9, basemap=ipyleaflet.basemaps.OpenStreetMap.Mapnik)    \n",
    "display(results_map)\n",
    "if open_image.RasterCount == 1:\n",
    "    file = '%s/%s_processed_%s.json' % (GeoJSON_path, os.path.splitext(input_name)[0], polarisations)\n",
    "    with open(file, 'r') as f:\n",
    "        data_json = json.load(f) \n",
    "    mask = ipyleaflet.GeoJSON(data = data_json, name = 'Flood Mask', style = {'color':'blue', 'opacity':'1', 'fillColor':'blue', 'fillOpacity':'1', 'weight':'0.8'})\n",
    "    results_map.add_layer(mask)\n",
    "    results_map.center = (mask.data['features'][0]['geometry']['coordinates'][0][0][1],\n",
    "                          mask.data['features'][0]['geometry']['coordinates'][0][0][0])\n",
    "else:\n",
    "    file_VV = '%s/%s_processed_VV.json' % (GeoJSON_path, os.path.splitext(input_name)[0])\n",
    "    with open(file_VV, 'r') as f_VV:\n",
    "        data_json_VV = json.load(f_VV)\n",
    "    mask_VV = ipyleaflet.GeoJSON(data = data_json_VV, name = 'Flood Mask: VV', style = {'color':'red', 'opacity':'1', 'fillColor':'red', 'fillOpacity':'1', 'weight':'0.8'})\n",
    "    results_map.add_layer(mask_VV)\n",
    "    results_map.center = (mask_VV.data['features'][0]['geometry']['coordinates'][0][0][1],\n",
    "                          mask_VV.data['features'][0]['geometry']['coordinates'][0][0][0])  \n",
    "    file_VH = '%s/%s_processed_VH.json' % (GeoJSON_path, os.path.splitext(input_name)[0])\n",
    "    with open(file_VH, 'r') as f_VH:\n",
    "        data_json_VH = json.load(f_VH)\n",
    "    mask_VH = ipyleaflet.GeoJSON(data = data_json_VH, name = 'Flood Mask: VH', style = {'color':'blue', 'opacity':'1', 'fillColor':'blue', 'fillOpacity':'1', 'weight':'0.8'})\n",
    "    results_map.add_layer(mask_VH)\n",
    "results_map.add_control(ipyleaflet.FullScreenControl())\n",
    "results_map.add_control(ipyleaflet.LayersControl(position='topright'))\n",
    "results_map.add_control(ipyleaflet.ScaleControl(position='bottomleft'))\n",
    "\n",
    "print('Plotting results done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
